# Stage 2 Memory Optimization Guide

## ë©”ëª¨ë¦¬ ì‚¬ìš© ë¶„ì„

### ì´ë¡ ì  ê³„ì‚° vs ì‹¤ì œ

**ì´ë¡  (ëª¨ë¸ íŒŒë¼ë¯¸í„°ë§Œ):**
```
Qwen2.5-VL-3B (bfloat16):  3B Ã— 2 bytes = 6GB
LoRA adapters (rank=8):     30M Ã— 2 bytes = 0.06GB
Sensor + Action Expert:     30M Ã— 2 bytes = 0.06GB
--------------------------------------------------
Total Model:                                6.12GB
```

**ì‹¤ì œ (í•™ìŠµ ì‹œ):**
```
Model Parameters:                           6.12GB
Optimizer States (AdamW for 60M params):    0.48GB
Gradients (for trainable params):           0.12GB
Activations (ê°€ì¥ í° ë¶€ë¶„!):                10-15GB
--------------------------------------------------
Total Training Memory:                      ~17-22GB per GPU
```

### Activation Memoryê°€ í° ì´ìœ 

1. **Vision Encoder Activations**
   - 2 images Ã— 1000+ patches per image = 2000+ tokens
   - ê° layerë§ˆë‹¤ ì €ì¥: 28 layers Ã— 2000 tokens Ã— 3072 dim Ã— 2 bytes
   - **~0.3GB per layer** â†’ Total **~10GB**

2. **Gradient Computation**
   - Backward passë¥¼ ìœ„í•´ ëª¨ë“  intermediate activation ë³´ê´€
   - Gradient checkpointing ì—†ìœ¼ë©´ **2ë°° ë©”ëª¨ë¦¬** í•„ìš”

## ì ìš©ëœ ìµœì í™”

### 1. Gradient Checkpointing âœ…

**íš¨ê³¼**: ë©”ëª¨ë¦¬ **50-70% ì ˆê°**

```python
# ìë™ ì ìš©ë¨
self.vl_model.gradient_checkpointing_enable()
```

**ì‘ë™ ì›ë¦¬:**
- Forward pass: activation ì¼ë¶€ë§Œ ì €ì¥ (checkpoints)
- Backward pass: í•„ìš”í•  ë•Œ ì¬ê³„ì‚°
- Trade-off: ë©”ëª¨ë¦¬â†“ ì†ë„â†“(ì•½ 20%)

### 2. Hybrid Caching Strategy âœ…

**ë¬¸ì œ**: `cache_mode="off"` â†’ ë§¤ iterationë§ˆë‹¤ VL forward pass

**í•´ê²°**: `cache_mode="on"` + ì£¼ê¸°ì  rebuild

```python
# Stage 2ì—ì„œë„ cache ì‚¬ìš©
self.cache_mode = "on"
self.cache_rebuild_interval = 5  # 5 epochë§ˆë‹¤ rebuild
```

**íš¨ê³¼**:
- VL forward pass: ë§¤ step â†’ ë§¤ 5 epoch
- ë©”ëª¨ë¦¬: **-10GB**
- ì†ë„: **10-20ë°° ë¹ ë¦„**

**ì£¼ì˜ì‚¬í•­**:
- LoRA ê°€ì¤‘ì¹˜ê°€ ì—…ë°ì´íŠ¸ë˜ë¯€ë¡œ cacheë„ ì£¼ê¸°ì  ê°±ì‹  í•„ìš”
- ë„ˆë¬´ ìì£¼ rebuild: ëŠë¦¼
- ë„ˆë¬´ ë“œë¬¼ê²Œ rebuild: stale features

### 3. Batch Size ì¡°ì •

**ê¶Œì¥ ì„¤ì •:**

| GPU VRAM | Batch Size | Grad Accum | Effective Batch |
|----------|------------|------------|-----------------|
| 24GB     | 2          | 16         | 32              |
| 24GB     | 4          | 8          | 32              |
| 40GB     | 4          | 8          | 32              |
| 80GB     | 8          | 4          | 32              |

### 4. LoRA Rank ì¡°ì •

**ë©”ëª¨ë¦¬ vs ì„±ëŠ¥ Trade-off:**

| LoRA Rank | Memory | ì„±ëŠ¥ | ê¶Œì¥ |
|-----------|--------|------|------|
| r=4       | ìµœì†Œ   | ë‚®ìŒ | âŒ |
| r=8       | ë‚®ìŒ   | ì¤‘ê°„ | âœ… (24GB GPU) |
| r=16      | ì¤‘ê°„   | ë†’ìŒ | âœ… (40GB GPU) |
| r=32      | ë†’ìŒ   | ìµœê³  | âŒ (overkill) |

### 5. Mixed Precision Training

**ìë™ ì ìš©:**
```python
with torch.autocast("cuda", dtype=torch.bfloat16):
    outputs = model(...)
```

**íš¨ê³¼**: ë©”ëª¨ë¦¬ ~30% ì ˆê°

## ì‹¤í–‰ ëª…ë ¹ì–´

### ìµœì í™”ëœ Stage 2 í•™ìŠµ (24GB GPU)

```bash
torchrun --nproc_per_node=4 \
    training/A5st_VLA_TRAIN_Diffusion_with_sensor.py \
    --dataset_dir /home/najo/NAS/VLA/dataset \
    --training-stage stage2 \
    --stage1-checkpoint checkpoints/diffusion_epoch20.pt \
    --finetune-vl lora \
    --lora-r 8 \
    --lora-alpha 16 \
    --lora-dropout 0.05 \
    --epochs 10 \
    --batch_size 2 \
    --grad_accum 16 \
    --lr 1e-4 \
    --vl-lr 1e-5
```

**ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:**
- Model: 6GB
- Optimizer: 0.5GB
- Gradients: 0.1GB
- Activations (with checkpointing): 5-7GB
- Batch overhead: 2-3GB
- **Total: ~14-17GB per GPU** âœ…

### ë” ê³µê²©ì ì¸ ìµœì í™” (í•„ìš”ì‹œ)

```bash
# Batch size 1 + ë” í° gradient accumulation
--batch_size 1 \
--grad_accum 32 \
--lora-r 4
```

## Cache Rebuild ì „ëµ

### Training Scriptì—ì„œ êµ¬í˜„ ì˜ˆì‹œ

```python
for epoch in range(start_epoch, args.epochs):
    # Rebuild cache every N epochs for LoRA
    if epoch > 0 and epoch % 5 == 0 and args.finetune_vl == "lora":
        if rank == 0:
            print(f"\nğŸ”„ Rebuilding VL cache (Epoch {epoch})...")

        # Clear old cache
        model.module.clear_cache()

        # Rebuild with current LoRA weights
        model.module.build_cache(train_loader)

        dist.barrier()

        if rank == 0:
            print("âœ… Cache rebuilt!\n")

    # Normal training...
    train_epoch(...)
```

## ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§

### GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸

```bash
# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# ìƒì„¸ ë¶„ì„
python -m torch.utils.bottleneck training_script.py
```

### PyTorch ë©”ëª¨ë¦¬ ì¶”ì 

```python
import torch

# í•™ìŠµ ì‹œì‘ ì „
torch.cuda.reset_peak_memory_stats()

# í•™ìŠµ ì¤‘...

# í•™ìŠµ í›„
max_memory = torch.cuda.max_memory_allocated() / 1024**3
print(f"Peak GPU Memory: {max_memory:.2f} GB")
```

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### OOM ë°œìƒ ì‹œ ì²´í¬ë¦¬ìŠ¤íŠ¸

1. âœ… Gradient checkpointing í™œì„±í™”ë˜ì—ˆë‚˜?
   ```python
   # ëª¨ë¸ ì´ˆê¸°í™” ë¡œê·¸ì—ì„œ í™•ì¸
   # "âœ… Gradient checkpointing enabled for VL model"
   ```

2. âœ… Cache ëª¨ë“œê°€ "on"ì¸ê°€?
   ```python
   # ë¡œê·¸ì—ì„œ í™•ì¸
   # "ğŸ’¾ Using cache mode with periodic rebuild"
   ```

3. âœ… Batch sizeê°€ ì ì ˆí•œê°€?
   - 24GB: batch_size â‰¤ 2
   - 40GB: batch_size â‰¤ 4

4. âœ… LoRA rankê°€ ë„ˆë¬´ í¬ì§€ ì•Šë‚˜?
   - 24GB: rank â‰¤ 8
   - 40GB: rank â‰¤ 16

5. âœ… ë„ˆë¬´ ë§ì€ GPU ì‚¬ìš©?
   - DDPëŠ” ê° GPUê°€ full model ë³µì‚¬ë³¸ ë³´ìœ 
   - 2 GPUë§Œ ì‚¬ìš©í•˜ë©´ ê° GPU ë©”ëª¨ë¦¬ ì—¬ìœ  ì¦ê°€

### ì—¬ì „íˆ OOM ë°œìƒí•˜ë©´

**Option 1: CPU Offloading (ëŠë¦¬ì§€ë§Œ ì•ˆì „)**
```python
# ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë ˆì´ì–´ë¥¼ CPUë¡œ ì´ë™
model.vl_model.to("cpu")
# Forward pass ì‹œì—ë§Œ GPUë¡œ ì´ë™
```

**Option 2: Deepspeed ZeRO Stage 2/3**
```bash
# Optimizer statesë¥¼ ì—¬ëŸ¬ GPUì— ë¶„ì‚°
torchrun --nproc_per_node=4 \
    training/A5st_VLA_TRAIN_Diffusion_with_sensor.py \
    --deepspeed \
    --deepspeed_config ds_config.json
```

## ì„±ëŠ¥ ë¹„êµ

### Before Optimization

```
Batch Size: 4
Memory per GPU: 23.5/24 GB (OOM!)
Speed: N/A (crashes)
```

### After Optimization

```
Batch Size: 2
Memory per GPU: 16/24 GB âœ…
Speed: ~2.5 steps/sec
Effective Batch: 32 (grad_accum=16)
```

## ê¶Œì¥ í•™ìŠµ ìŠ¤ì¼€ì¤„

```python
# Epoch 1-5: Cache ì‚¬ìš© (ë¹ ë¦„, ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
cache_mode = "on"

# Epoch 5: Cache rebuild (LoRA ì—…ë°ì´íŠ¸ ë°˜ì˜)
rebuild_cache()

# Epoch 6-10: Cache ì‚¬ìš©
cache_mode = "on"

# Epoch 10: Cache rebuild
rebuild_cache()

# ...ê³„ì†
```

## ê²°ë¡ 

**24GB GPUì—ì„œ Stage 2 í•™ìŠµì€ ê°€ëŠ¥í•©ë‹ˆë‹¤!**

í•µì‹¬:
1. âœ… Gradient Checkpointing
2. âœ… Hybrid Caching
3. âœ… Appropriate Batch Size (2)
4. âœ… LoRA Rank (8)

ì´ ì„¤ì •ìœ¼ë¡œ **ë©”ëª¨ë¦¬ ~17GB**, **ì†ë„ 2-3 steps/sec** ë‹¬ì„± ê°€ëŠ¥í•©ë‹ˆë‹¤.
