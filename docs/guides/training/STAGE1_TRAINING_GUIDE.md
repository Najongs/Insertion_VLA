# Stage 1 Training Guide (Optimized)

## ê°œìš”

Stage 1ì€ VL backboneì„ frozen ìƒíƒœë¡œ ìœ ì§€í•˜ë©´ì„œ Sensor Encoderì™€ Action Expertë§Œ í•™ìŠµí•©ë‹ˆë‹¤.
**LoRA ì—†ì´ Stage 1ë§Œ ì§„í–‰í•˜ëŠ” ê²½ìš°** ì´ ê°€ì´ë“œë¥¼ ë”°ë¥´ì„¸ìš”.

## ìµœì í™” ì ìš©

ë‹¤ìŒ ìµœì í™”ê°€ ê¸°ë³¸ê°’ìœ¼ë¡œ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤:

1. **ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ: 640x360**
   - VLM ì¶”ë¡  ì‹œê°„ 3.9ë°° ë‹¨ì¶• (1487ms â†’ 381ms @ 5 views)
   - ìºì‹œ ë¹Œë“œ ì‹œê°„ë„ ë‹¨ì¶•

2. **ì„¼ì„œ ìœˆë„ìš°: 65 samples**
   - 100ms @ 650Hz (ê¸°ì¡´ 650 samples = 1ì´ˆ)
   - ì‹¤ì‹œê°„ ì œì–´ì— ì í•©í•œ ì§§ì€ ìœˆë„ìš°

3. **Weighted Sampling**
   - Priority datasets (Needle_insertion_eye_trocar, White_silicone_white_circle): **2x ê°€ì¤‘ì¹˜**
   - Regular datasets (OCT_insertion, part1): 1x ê°€ì¤‘ì¹˜

## í•™ìŠµ ë‹¨ê³„

### 1ë‹¨ê³„: VL Feature Cache ë¹Œë“œ

ë¨¼ì € VL featuresë¥¼ ìºì‹œë¡œ ì €ì¥í•©ë‹ˆë‹¤ (í•™ìŠµ ì†ë„ í–¥ìƒ).

```bash
# 4ê°œ GPU ì‚¬ìš© ì˜ˆì‹œ
torchrun --nproc_per_node=4 \
    training/A5st_VLA_TRAIN_VL_Lora_with_sensor.py \
    --mode cache \
    --sensor-enabled \
    --sensor-window-size 65 \
    --image-resize-height 360 \
    --image-resize-width 640
```

**ì˜ˆìƒ ì‹œê°„:**
- ê¸°ì¡´ (1280x720): ~2-3ì‹œê°„
- ìµœì í™” (640x360): ~30-45ë¶„ (ì•½ 4ë°° ë¹ ë¦„)

**ì¶œë ¥ ì˜ˆì‹œ:**
```
ğŸš€ [Rank 0] Running in CACHE mode on 4 GPUs
ğŸ”¬ Stage 1 Optimized Training
   - Sensor enabled: True
   - Sensor window: 65 samples (100ms @ 650Hz)
   - Image resize: 640x360
   - Priority datasets: 2x weight (Needle_insertion, White_silicone)

ğŸ“¦ Building integrated dataset...
âœ… [Priority 2x] Added: recv_all_20251027_170308 (500 samples, WITH sensor)
âœ… [Priority 2x] Added: recv_all_20251028_141523 (600 samples, WITH sensor)
âœ… Added: Captures1 (800 samples, NO sensor)
âœ… Added: ZED_Captures_4th (700 samples, NO sensor)

ğŸ“Š Total dataset size: 4400 samples (priority datasets counted 2x)

â³ Initializing VL-only model for cache building...
   Image resize: 640x360
   ğŸ“ Cache will use 640x360 images (230,400 pixels)

ğŸ”„ Building VL cache (distributed)...
[Rank 0] Processing batch 100/1100...
...
âœ… Cache build complete. You can now run training with --mode train.
```

### 2ë‹¨ê³„: Stage 1 í•™ìŠµ

ìºì‹œê°€ ì¤€ë¹„ë˜ë©´ Sensor Encoderì™€ Action Expertë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.

```bash
# 4ê°œ GPU ì‚¬ìš© ì˜ˆì‹œ
torchrun --nproc_per_node=4 \
    training/A5st_VLA_TRAIN_VL_Lora_with_sensor.py \
    --mode train \
    --sensor-enabled \
    --sensor-window-size 65 \
    --image-resize-height 360 \
    --image-resize-width 640 \
    --finetune-vl none \
    --training-stage stage1 \
    --batch-size 1 \
    --grad-accum-steps 8 \
    --lr 1e-4 \
    --sensor-lr 5e-4 \
    --sensor-loss-weight 2.0
```

**ì£¼ìš” íŒŒë¼ë¯¸í„°:**

| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… |
|---------|-----|------|
| `--mode` | train | í•™ìŠµ ëª¨ë“œ |
| `--finetune-vl` | none | VL backbone frozen (LoRA ì—†ìŒ) |
| `--training-stage` | stage1 | Stage 1 í•™ìŠµ |
| `--sensor-window-size` | 65 | 100ms @ 650Hz |
| `--image-resize-height` | 360 | ì´ë¯¸ì§€ ë†’ì´ |
| `--image-resize-width` | 640 | ì´ë¯¸ì§€ ë„ˆë¹„ |
| `--sensor-lr` | 5e-4 | Sensor encoder learning rate |
| `--sensor-loss-weight` | 2.0 | Sensor ë°ì´í„° loss ê°€ì¤‘ì¹˜ |
| `--batch-size` | 1 | GPUë‹¹ ë°°ì¹˜ í¬ê¸° |
| `--grad-accum-steps` | 8 | Gradient accumulation |

**ì¶œë ¥ ì˜ˆì‹œ:**
```
ğŸš€ [Rank 0] Running in TRAIN mode on 4 GPUs
ğŸ”¬ Stage 1 Optimized Training
   - Sensor enabled: True
   - Sensor window: 65 samples (100ms @ 650Hz)
   - Fusion strategy: concat
   - Sensor LR: 0.0005
   - Sensor loss weight: 2.0
   - Image resize: 640x360
   - Priority datasets: 2x weight (Needle_insertion, White_silicone)

ğŸ“¦ Building integrated dataset...
âœ… [Priority 2x] Added: recv_all_20251027_170308 (500 samples, WITH sensor)
âœ… [Priority 2x] Added: recv_all_20251028_141523 (600 samples, WITH sensor)
âœ… Added: Captures1 (800 samples, NO sensor)
âœ… Added: ZED_Captures_4th (700 samples, NO sensor)

ğŸ“Š Total dataset size: 4400 samples
   Train: 4180 samples
   Val: 220 samples

â³ Initializing full QwenVLA model for training...
ğŸš€ Loading Trainable Qwen-VL-Sensor Model
   VL Fine-tuning: none
   Sensor Enabled: True
   Fusion Strategy: concat
   ğŸ“ Image resize: 640x360 (230,400 pixels)
ğŸ§Š Using frozen VL backbone.
âœ… Model loaded

ğŸ’¡ Trainable parameters:
   - Action Expert: 12.3M
   - Sensor Encoder: 8.5M
   - Total trainable: 20.8M
   - Total frozen: 3.0B (VL backbone)

Epoch 1/100:
  [Step 100/522] loss: 0.012456, lr: 1.2e-5, grad: 0.82, sensor: 120/200
  [Step 200/522] loss: 0.009123, lr: 2.4e-5, grad: 0.65, sensor: 240/400
  ...

ğŸ“Š Epoch 1 | Train: 0.008234 | Val: 0.009123
ğŸ† [Best] Validation improved â†’ saved to ./checkpoints/qwen_vla_sensor_best.pt
```

## í•™ìŠµ ì‹œê°„ ì˜ˆìƒ

**GPU: 4x RTX 4090**
- Epochë‹¹: ~15-20ë¶„
- Total (100 epochs): ~25-30ì‹œê°„

**GPU: 4x A100**
- Epochë‹¹: ~10-12ë¶„
- Total (100 epochs): ~16-20ì‹œê°„

## ì²´í¬í¬ì¸íŠ¸

í•™ìŠµ ì¤‘ ì²´í¬í¬ì¸íŠ¸ê°€ ìë™ìœ¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤:

- `./checkpoints/qwen_vla_sensor.pt`: ìµœì‹  ì²´í¬í¬ì¸íŠ¸ (ë§¤ epoch)
- `./checkpoints/qwen_vla_sensor_best.pt`: Best validation loss
- `./checkpoints/qwen_vla_sensor_final.pt`: ìµœì¢… ì²´í¬í¬ì¸íŠ¸

## í•™ìŠµ ì¬ê°œ

í•™ìŠµì´ ì¤‘ë‹¨ëœ ê²½ìš°, ê°™ì€ ëª…ë ¹ì–´ë¡œ ìë™ ì¬ê°œë©ë‹ˆë‹¤:

```bash
# ìë™ìœ¼ë¡œ ./checkpoints/qwen_vla_sensor.ptì—ì„œ ì¬ê°œ
torchrun --nproc_per_node=4 \
    training/A5st_VLA_TRAIN_VL_Lora_with_sensor.py \
    --mode train \
    --sensor-enabled \
    --sensor-window-size 65 \
    --image-resize-height 360 \
    --image-resize-width 640 \
    --finetune-vl none \
    --training-stage stage1
```

## ê²€ì¦

í•™ìŠµì´ ì™„ë£Œë˜ë©´ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•˜ì—¬ ì¶”ë¡  í…ŒìŠ¤íŠ¸:

```python
from models.model_with_sensor import Not_freeze_QwenVLAWithSensor

# ëª¨ë¸ ë¡œë“œ
model = Not_freeze_QwenVLAWithSensor(
    vl_model_name="Qwen/Qwen2.5-VL-3B-Instruct",
    action_dim=7,
    horizon=8,
    finetune_vl="none",
    sensor_enabled=True,
    sensor_temporal_length=65,
    image_resize_height=360,
    image_resize_width=640,
)

# ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
checkpoint = torch.load("./checkpoints/qwen_vla_sensor_best.pt")
model.load_state_dict(checkpoint["model_state_dict"])
model.eval()

# ì¶”ë¡ 
with torch.no_grad():
    actions, _ = model(
        text_inputs=["Insert needle into target"],
        image_inputs=[image_paths],  # 5 views
        z_chunk=torch.zeros(1, 8, 7),
        sensor_data=sensor_window,  # (1, 65, 1026)
    )
```

## WandB ëª¨ë‹ˆí„°ë§

í•™ìŠµ ì¤‘ WandBì— ìë™ìœ¼ë¡œ ë¡œê·¸ê°€ ê¸°ë¡ë©ë‹ˆë‹¤:

- `train/loss_step`: Stepë³„ í•™ìŠµ loss
- `train/loss_epoch`: Epochë³„ í‰ê·  loss
- `val/loss_epoch`: Validation loss
- `train/lr`: Learning rate
- `train/grad_norm`: Gradient norm
- `train/sensor_samples`: Sensor ë°ì´í„° ì‚¬ìš© ìƒ˜í”Œ ìˆ˜
- `train/nonsensor_samples`: Sensor ì—†ëŠ” ìƒ˜í”Œ ìˆ˜

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. CUDA Out of Memory

ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°:
```bash
--batch-size 1 --grad-accum-steps 16  # ë” ì‘ì€ ë°°ì¹˜, ë” ë§ì€ accumulation
```

### 2. Cache Miss Error

ìºì‹œë¥¼ ë‹¤ì‹œ ë¹Œë“œ:
```bash
rm -rf /home/najo/NAS/VLA/dataset/cache/qwen_vl_features/*
# ê·¸ ë‹¤ìŒ cache ëª¨ë“œ ë‹¤ì‹œ ì‹¤í–‰
```

### 3. Dataset ë¡œë”© ì‹¤íŒ¨

ë°ì´í„° ê²½ë¡œ í™•ì¸:
```python
# A5st_VLA_TRAIN_VL_Lora_with_sensor.pyì—ì„œ ìˆ˜ì •
priority_dataset_dirs = [
    "YOUR_PATH/White_silicone_white_circle/recv_all_*",
    "YOUR_PATH/Needle_insertion_eye_trocar/recv_all_*",
]
```

## ë‹¤ìŒ ë‹¨ê³„

Stage 1 í•™ìŠµ ì™„ë£Œ í›„:

1. **Best checkpoint ì„ íƒ**: `qwen_vla_sensor_best.pt` ì‚¬ìš© ê¶Œì¥
2. **ì‹¤ì œ ë¡œë´‡ í…ŒìŠ¤íŠ¸**: `examples/example_sensor_vla_usage.py` ì°¸ê³ 
3. **ì¶”ê°€ fine-tuning í•„ìš” ì‹œ**: Stage 2 (LoRA) ê³ ë ¤

## ìš”ì•½

âœ… **ìµœì í™” ì ìš© ì™„ë£Œ:**
- 640x360 ì´ë¯¸ì§€ â†’ 4ë°° ë¹ ë¥¸ í•™ìŠµ
- 65 samples ì„¼ì„œ ìœˆë„ìš° â†’ ì‹¤ì‹œê°„ ì œì–´ ê°€ëŠ¥
- Priority datasets 2x â†’ ë” ë‚˜ì€ ì„±ëŠ¥

âœ… **Stage 1 only:**
- Frozen VL backbone (3B parameters)
- Trainable Sensor + Action Expert (20.8M parameters)
- LoRA ì—†ì´ ë¹ ë¥¸ í•™ìŠµ ì™„ë£Œ

ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤! ğŸš€
