# Real-time Inference Guide

## ê°œìš”

ìµœì í™”ëœ ë¹„ë™ê¸° VLA ëª¨ë¸ì„ ì´ìš©í•œ ì‹¤ì‹œê°„ ì¶”ë¡  ê°€ì´ë“œì…ë‹ˆë‹¤.

## ì‹œìŠ¤í…œ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Sources                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Robot Sender â”‚ Camera Senderâ”‚ Sensor Sender                â”‚
â”‚ (ZMQ PUB)    â”‚ (ZMQ PUSH)   â”‚ (UDP)                        â”‚
â”‚ Port 5556    â”‚ Port 5555    â”‚ Port 9999                    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚              â”‚              â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚    Async_inference_receiver.py              â”‚
       â”‚                                             â”‚
       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
       â”‚  â”‚  VLM Thread (Background)            â”‚   â”‚
       â”‚  â”‚  - Updates VL features ~2.6Hz       â”‚   â”‚
       â”‚  â”‚  - Time: ~381ms @ 640x360           â”‚   â”‚
       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
       â”‚                                             â”‚
       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
       â”‚  â”‚  Action Expert Thread (10Hz)        â”‚   â”‚
       â”‚  â”‚  - Predicts actions every 100ms     â”‚   â”‚
       â”‚  â”‚  - Reuses VL features 4x            â”‚   â”‚
       â”‚  â”‚  - Time: ~20-30ms per prediction    â”‚   â”‚
       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
              Action Commands (10Hz)
              80 actions/sec total
```

## ìµœì í™” ì ìš©ì‚¬í•­

1. **ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ: 640x360**
   - VLM ì¶”ë¡  ì‹œê°„: 1487ms â†’ 381ms (3.9ë°° ë¹ ë¦„)
   - ìë™ ë¦¬ì‚¬ì´ì¦ˆ ì ìš©

2. **ì„¼ì„œ ìœˆë„ìš°: 65 samples**
   - 100ms @ 650Hz (ê¸°ì¡´ 650 samples = 1ì´ˆ)
   - ì‹¤ì‹œê°„ ì œì–´ì— ì í•©

3. **ë¹„ë™ê¸° VL-Action ë¶„ë¦¬**
   - VLM: Background thread (~2.6Hz)
   - Action Expert: Main thread (10Hz)
   - VL features 4ë²ˆ ì¬ì‚¬ìš©

## ì‚¬ìš© ë°©ë²•

### 1ë‹¨ê³„: ë°ì´í„° ì†¡ì‹  ì‹œì‘

#### Robot Sender (ê¸°ì¡´ ì½”ë“œ ì‚¬ìš© ê°€ëŠ¥)

```bash
# í„°ë¯¸ë„ 1: Robotì—ì„œ ì‹¤í–‰
cd /home/najo/NAS/VLA/Insertion_VLA/Make_dataset
python Robot_sender.py --robot on
```

**ì¶œë ¥:**
```
âœ… Starting robot data sampling to ./dataset/Robot_Data/robot_data_XXX.csv at 100.0 Hz...
âœ… ZMQ Publisher initialized on *:5556
ğŸš€ Robot control thread started!
```

#### Camera Sender (Jetsonì—ì„œ ì‹¤í–‰)

```bash
# í„°ë¯¸ë„ 2: Jetsonì—ì„œ ì‹¤í–‰
python Improved_Jetson_sender.py
```

**ì¶œë ¥:**
```
âœ… Found 4 ZED cameras + 1 OAK camera
âœ… ZMQ PUSH socket connected to tcp://10.130.41.XXX:5555
ğŸ¥ Capturing at 30 FPS...
```

#### Sensor Sender (ë³„ë„ ë¨¸ì‹ ì—ì„œ ì‹¤í–‰)

ì„¼ì„œ ë°ì´í„°ëŠ” UDPë¡œ ìë™ ì „ì†¡ë©ë‹ˆë‹¤ (ê¸°ì¡´ ì„¤ì • ì‚¬ìš©).

### 2ë‹¨ê³„: ì¶”ë¡  ì‹œì‘

#### ê¸°ë³¸ ì‚¬ìš© (ì¶”ë¡ ë§Œ)

```bash
# í„°ë¯¸ë„ 3: ì¶”ë¡  ì„œë²„ì—ì„œ ì‹¤í–‰
cd /home/najo/NAS/VLA/Insertion_VLA/Make_dataset
python Async_inference_receiver.py \
    --checkpoint ../checkpoints/qwen_vla_sensor_best.pt
```

**ì˜ˆìƒ ì¶œë ¥:**
```
================================================================================
Initializing Async VLA Inference Engine
================================================================================
Device: cuda
Model: Qwen/Qwen2.5-VL-3B-Instruct
Image Resize: 640x360
Sensor Window: 65 samples (100ms @ 650Hz)
Action Expert: 10.0 Hz
VL Reuse: 4x
Fusion Strategy: concat
Loading checkpoint: ../checkpoints/qwen_vla_sensor_best.pt
âœ… Checkpoint loaded successfully
================================================================================

âœ… Camera PULL listening on port 5555
âœ… Robot SUB connected to 10.130.41.111:5556
âœ… Sensor UDP Receiver started on port 9999
â³ Calibrating sensor clock offset (first 50 batches)...

âœ… Sensor Clock Offset Calibrated: 12.3 ms

================================================================================
Async Real-time Inference Started
================================================================================
Action Expert: 10.0 Hz (every 100ms)
VL Update: ~2.6 Hz (VL features reused 4x)
Image Resolution: 640x360
Sensor Window: 65 samples (100ms)
Device: cuda
Data Saving: Disabled

Waiting for data from all sources...
Press Ctrl+C to stop

ğŸ”„ [VL Update #1] Completed in 385ms
[ACTION #1] VL_reuse=1/4 | Actions[0]: [0.123, -0.045, 0.089, ...] | Time: 24.3ms | Sensor: 65/65
[ACTION #2] VL_reuse=2/4 | Actions[0]: [0.125, -0.043, 0.091, ...] | Time: 22.1ms | Sensor: 65/65
[ACTION #3] VL_reuse=3/4 | Actions[0]: [0.127, -0.041, 0.093, ...] | Time: 23.5ms | Sensor: 65/65
[ACTION #4] VL_reuse=4/4 | Actions[0]: [0.129, -0.039, 0.095, ...] | Time: 21.8ms | Sensor: 65/65
ğŸ”„ [VL Update #2] Completed in 378ms
[ACTION #5] VL_reuse=1/4 | Actions[0]: [0.131, -0.037, 0.097, ...] | Time: 24.1ms | Sensor: 65/65
...

--- Status (14:32:15) ---
VL Updates: 25 | VL avg: 381ms
Actions: 100 | Action avg: 23.2ms
Images recv: View1:3000, View2:3000, View3:3000, View4:3000, View5:3000
Sensor buffer: 65/65
Robot: J1=45.23Â°, Px=123.45mm
```

#### ë°ì´í„° ì €ì¥ ëª¨ë“œ (ë””ë²„ê¹…ìš©)

```bash
python Async_inference_receiver.py \
    --checkpoint ../checkpoints/qwen_vla_sensor_best.pt \
    --save-data
```

ì €ì¥ë˜ëŠ” ë°ì´í„°:
- ì´ë¯¸ì§€: `async_inference_YYYYMMDD_HHMMSS/View1-5/*.jpg`
- ë¡œë´‡ ìƒíƒœ: `robot_state_YYYYMMDD_HHMMSS.csv`
- ì„¼ì„œ ë°ì´í„°: `sensor_data_YYYYMMDD_HHMMSS.npz`
- ì¶”ë¡  ê²°ê³¼: `inference_results_YYYYMMDD_HHMMSS.json`

#### VL Reuse ì»¤ìŠ¤í„°ë§ˆì´ì§•

```bash
# VL featuresë¥¼ 3ë²ˆë§Œ ì¬ì‚¬ìš© (ë” ë¹ˆë²ˆí•œ VLM ì—…ë°ì´íŠ¸)
python Async_inference_receiver.py \
    --checkpoint ../checkpoints/qwen_vla_sensor_best.pt \
    --vl-reuse 3

# VL featuresë¥¼ 6ë²ˆ ì¬ì‚¬ìš© (ëœ ë¹ˆë²ˆí•œ VLM ì—…ë°ì´íŠ¸)
python Async_inference_receiver.py \
    --checkpoint ../checkpoints/qwen_vla_sensor_best.pt \
    --vl-reuse 6
```

## ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### ì£¼ìš” ì§€í‘œ

**VL Update (Background)**
- ëª©í‘œ: ~2.6 Hz (380ms ê°„ê²©)
- ì‹¤ì œ: 375-390ms (ì •ìƒ)
- ë¬¸ì œ: >500ms (GPU ê³¼ë¶€í•˜ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ì§€ì—°)

**Action Prediction (10Hz)**
- ëª©í‘œ: ~100ms ê°„ê²© (10Hz)
- Action Expert ì‹œê°„: 20-30ms (ì •ìƒ)
- ë¬¸ì œ: >100ms (GPU ê³¼ë¶€í•˜)

**ì„¼ì„œ ë²„í¼**
- ì •ìƒ: 60-65/65 samples
- ì£¼ì˜: <30/65 samples (ì„¼ì„œ ë°ì´í„° ìœ ì‹¤)

### ë¡œê·¸ í•´ì„

```
[ACTION #42] VL_reuse=2/4 | Actions[0]: [0.123, -0.045, 0.089, ...] | Time: 24.3ms | Sensor: 65/65
```

- `ACTION #42`: 42ë²ˆì§¸ action ì˜ˆì¸¡
- `VL_reuse=2/4`: í˜„ì¬ VL features 2ë²ˆì§¸ ì¬ì‚¬ìš© ì¤‘ (ì´ 4ë²ˆ ì¬ì‚¬ìš© ì˜ˆì •)
- `Actions[0]`: ì²« ë²ˆì§¸ horizonì˜ action ê°’
- `Time: 24.3ms`: Action Expert ì¶”ë¡  ì‹œê°„ (ëª©í‘œ: <100ms)
- `Sensor: 65/65`: ì„¼ì„œ ë²„í¼ ìƒíƒœ (ì¶©ë¶„íˆ ì°¼ìŒ)

```
ğŸ”„ [VL Update #5] Completed in 385ms
```

- VL features ì—…ë°ì´íŠ¸ ì™„ë£Œ
- 385ms ì†Œìš” (ì •ìƒ ë²”ìœ„: 375-400ms)

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. VL Updateê°€ ë„ˆë¬´ ëŠë¦¼ (>500ms)

**ì¦ìƒ:**
```
ğŸ”„ [VL Update #10] Completed in 650ms
```

**ì›ì¸:**
- GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
- ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ê°€ GPU ì‚¬ìš© ì¤‘
- ì´ë¯¸ì§€ í•´ìƒë„ê°€ 640x360ë¡œ ë¦¬ì‚¬ì´ì¦ˆë˜ì§€ ì•ŠìŒ

**í•´ê²°:**
```bash
# GPU ì‚¬ìš©ëŸ‰ í™•ì¸
nvidia-smi

# ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
pkill -f "python.*train"

# ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ í™•ì¸ (ì½”ë“œ ë‚´ë¶€ì—ì„œ ìë™)
```

### 2. Action Expertê°€ ë„ˆë¬´ ëŠë¦¼ (>50ms)

**ì¦ìƒ:**
```
[ACTION #10] ... | Time: 89.5ms | ...
```

**ì›ì¸:**
- GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
- ì„¼ì„œ ìœˆë„ìš°ê°€ 650 samplesë¡œ ì„¤ì •ë¨ (ì˜ëª»ëœ ì„¤ì •)

**í•´ê²°:**
```bash
# ëª¨ë¸ ì¬ë¡œë“œ í™•ì¸
# ì„¼ì„œ ìœˆë„ìš°ê°€ 65ë¡œ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
```

### 3. ì„¼ì„œ ë²„í¼ê°€ ê³„ì† ë¹„ì–´ìˆìŒ

**ì¦ìƒ:**
```
Sensor buffer: 5/65
```

**ì›ì¸:**
- ì„¼ì„œ UDP ë°ì´í„° ìˆ˜ì‹  ì•ˆ ë¨
- ì„¼ì„œ senderê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ
- ë°©í™”ë²½ ì°¨ë‹¨

**í•´ê²°:**
```bash
# UDP í¬íŠ¸ í™•ì¸
sudo netstat -ulpn | grep 9999

# ë°©í™”ë²½ í™•ì¸
sudo ufw status
sudo ufw allow 9999/udp

# ì„¼ì„œ sender ì¬ì‹œì‘
```

### 4. ì´ë¯¸ì§€ê°€ ìˆ˜ì‹ ë˜ì§€ ì•ŠìŒ

**ì¦ìƒ:**
```
[WAIT] VL Features: False | Images: False (0/5) | Sensor: True (65/65)
```

**ì›ì¸:**
- Camera senderê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ
- ZMQ ì—°ê²° ë¬¸ì œ
- ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ

**í•´ê²°:**
```bash
# ZMQ í¬íŠ¸ í™•ì¸
netstat -an | grep 5555

# Camera sender ì¬ì‹œì‘ (Jetson)
python Improved_Jetson_sender.py

# ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸
ping 10.130.41.XXX
```

## ì¶”ë¡  ê²°ê³¼ í™œìš©

### JSON í˜•ì‹

```json
[
  {
    "timestamp": 1234567890.123,
    "actions": [
      [0.123, -0.045, 0.089, 0.012, -0.034, 0.067, 0.5],  // Horizon 1
      [0.125, -0.043, 0.091, 0.014, -0.032, 0.069, 0.5],  // Horizon 2
      // ... (8 horizons total)
    ],
    "delta": [...],
    "inference_time": 0.0243,
    "vl_update_number": 12,
    "robot_state": {
      "joints": [45.2, 12.3, ...],
      "pose": [123.4, 56.7, ...],
      "timestamp": 1234567890.120
    }
  },
  ...
]
```

### Pythonìœ¼ë¡œ ê²°ê³¼ ì½ê¸°

```python
import json
import numpy as np

# Load results
with open("async_inference_20251029_143015/inference_results_20251029_143015.json") as f:
    results = json.load(f)

# Extract actions
for i, result in enumerate(results):
    timestamp = result['timestamp']
    actions = np.array(result['actions'])  # (8, 7)

    # Use first action in horizon
    first_action = actions[0]  # (7,)
    joints = first_action[:6]  # Joint commands
    gripper = first_action[6]  # Gripper command

    print(f"[{i}] Time: {timestamp:.3f} | Joints: {joints} | Gripper: {gripper:.2f}")
```

## ì„±ëŠ¥ ìš”ì•½

| í•­ëª© | ê°’ | ë¹„ê³  |
|------|-----|------|
| Action Expert ì£¼íŒŒìˆ˜ | 10 Hz | 100ms ê°„ê²© |
| VLM ì—…ë°ì´íŠ¸ ì£¼íŒŒìˆ˜ | ~2.6 Hz | 381ms @ 640x360 |
| VL feature reuse | 4x | 400msë§ˆë‹¤ ê°±ì‹  |
| ì´ action ì¶œë ¥ | 80 actions/sec | 10Hz Ã— 8 horizon |
| Action Expert ì‹œê°„ | 20-30ms | GPU ì—¬ìœ  |
| VLM ì¶”ë¡  ì‹œê°„ | ~381ms | 5 views @ 640x360 |
| ì„¼ì„œ ìœˆë„ìš° | 65 samples | 100ms @ 650Hz |
| ë©”ëª¨ë¦¬ ì‚¬ìš© | ~8GB GPU | ìµœì í™”ë¨ |

## ë‹¤ìŒ ë‹¨ê³„

1. âœ… **ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì¤€ë¹„**: Stage 1 í•™ìŠµ ì™„ë£Œ
2. âœ… **ì¶”ë¡  ì½”ë“œ ì‘ì„±**: Async_inference_receiver.py
3. **ì‹¤ì œ ë¡œë´‡ í…ŒìŠ¤íŠ¸**: ë¡œë´‡ì—ì„œ ë°ì´í„° ì†¡ìˆ˜ì‹  í…ŒìŠ¤íŠ¸
4. **ì„±ëŠ¥ íŠœë‹**: VL reuse count ì¡°ì •
5. **ì‹¤ì‹œê°„ ì œì–´**: Action commandsë¥¼ ë¡œë´‡ì— ì ìš©

ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤! ğŸš€
